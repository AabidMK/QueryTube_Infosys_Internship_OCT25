{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4063449-7df2-4de3-9a0c-96365e390d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7713b-8ef8-4acc-849b-da2828faf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "import sys \n",
    "from datetime import datetime\n",
    "import threading\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import json \n",
    "from urllib.parse import urlencode \n",
    "\n",
    "# Define the base directory for all file operations\n",
    "BASE_DIR = \"C:\\\\Users\\\\Pooja\\\\Downloads\"\n",
    "\n",
    "# ===== FIXED CONFIGURATION WITH MONITORING (Windows Paths) =====\n",
    "INPUT_CSV = os.path.join(BASE_DIR, \"youtube_data_metadata.xls - youtube_data_metadata.xls.csv\") \n",
    "OUTPUT_FOLDER = os.path.join(BASE_DIR, \"Output\")\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_FOLDER, \"all_video_details_and_transcripts.csv\") \n",
    "PROGRESS_LOG = os.path.join(OUTPUT_FOLDER, \"progress_log.txt\") \n",
    "\n",
    "# ==============================================================================\n",
    "# üí• PROXY RELIABILITY SETTINGS (Adjusted Cooldown for Flow Test) üí•\n",
    "# Cooldown reduced to 5 minutes to prevent very long initial waits that might be causing the script to bypass the main loop.\n",
    "# ==============================================================================\n",
    "PROXIES = [\n",
    "    # General high-anonymity proxies (often stable)\n",
    "    \"http://49.148.236.124:8080\", \"http://103.156.249.52:8080\", \"http://103.144.18.74:8080\", \n",
    "    \"http://185.82.99.148:9091\", \"http://103.165.155.69:8080\", \"http://103.117.192.174:80\", \n",
    "    \"http://190.107.224.150:3128\", \"http://103.144.18.98:8080\", \"http://134.122.116.174:8080\", \n",
    "    \"http://147.182.211.215:8080\", \"http://68.183.111.90:8080\", \"http://164.90.179.64:8080\",\n",
    "]\n",
    "\n",
    "# Optimized settings\n",
    "RETRIES = 5 \n",
    "MIN_DELAY = 10 # Reduced minimum delay\n",
    "MAX_DELAY = 30 # Reduced maximum delay\n",
    "REQUESTS_PER_PROXY = 1 \n",
    "COOLDOWN_TIME = 300 # 5 minutes (300 seconds) for faster testing\n",
    "REQUEST_TIMEOUT = 45 \n",
    "# ==============================================================================\n",
    "\n",
    "# Global monitoring variables\n",
    "monitoring_data = {\n",
    "    'total_videos': 0,\n",
    "    'processed_videos': 0,\n",
    "    'successful_transcripts': 0,\n",
    "    'failed_videos': 0,\n",
    "    'current_video': '',\n",
    "    'start_time': 0,\n",
    "    'eta_minutes': 0,\n",
    "    'current_status': 'Initializing...',\n",
    "    'proxy_stats': {},\n",
    "    'transcript_types': {'manual': 0, 'auto-generated': 0, 'unknown': 0},\n",
    "    'recent_errors': []\n",
    "}\n",
    "\n",
    "# --- Utility Functions (Only changed the Proxy Manager logic for faster start) ---\n",
    "\n",
    "def fetch_additional_proxies():\n",
    "    return []\n",
    "\n",
    "extra_proxies = fetch_additional_proxies()\n",
    "PROXIES.extend(extra_proxies)\n",
    "PROXIES = list(set([p for p in PROXIES if p.startswith('http://') and ':' in p]))\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear() \n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(PROGRESS_LOG, encoding='utf-8')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setFormatter(formatter)\n",
    "stream_handler.encoding = 'utf-8' \n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "class MonitoringProxyManager:\n",
    "    def __init__(self, proxies):\n",
    "        self.proxies = proxies\n",
    "        self.request_counts = {proxy: 0 for proxy in proxies}\n",
    "        self.failed_proxies = set()\n",
    "        self.last_used = {proxy: 0 for proxy in proxies} \n",
    "        self.success_rates = {proxy: 1.0 for proxy in proxies}\n",
    "\n",
    "    def get_next_proxy(self):\n",
    "        available_proxies = [p for p in self.proxies if p not in self.failed_proxies]\n",
    "\n",
    "        if not available_proxies:\n",
    "            logger.warning(\"‚ö†Ô∏è All proxies failed, resetting failed list...\")\n",
    "            self.failed_proxies.clear()\n",
    "            available_proxies = self.proxies\n",
    "\n",
    "        current_time = time.time()\n",
    "        ready_proxies = []\n",
    "        \n",
    "        # üí° FIX: Temporarily prioritize proxies that haven't been used at all\n",
    "        never_used_proxies = [p for p in available_proxies if self.last_used.get(p, 0) == 0]\n",
    "        if never_used_proxies:\n",
    "             ready_proxies = never_used_proxies\n",
    "        else:\n",
    "             # If all have been used, check cooldown\n",
    "             for p in available_proxies:\n",
    "                time_since_last = current_time - self.last_used.get(p, 0)\n",
    "                if time_since_last >= COOLDOWN_TIME or self.request_counts[p] < REQUESTS_PER_PROXY:\n",
    "                     ready_proxies.append(p)\n",
    "\n",
    "        if not ready_proxies:\n",
    "            # If all are cooling down, wait for the earliest one\n",
    "            wait_proxy = min(available_proxies, key=lambda p: self.last_used.get(p, 0))\n",
    "            time_since_last = current_time - self.last_used.get(wait_proxy, 0)\n",
    "            cooldown_remaining = COOLDOWN_TIME - time_since_last\n",
    "            if cooldown_remaining > 0:\n",
    "                monitoring_data['current_status'] = f\"üò¥ All proxies cooling down. Waiting {cooldown_remaining:.0f}s for next...\"\n",
    "                update_progress_display()\n",
    "                logger.info(f\"üïí All proxies cooling down, waiting {cooldown_remaining:.0f}s...\")\n",
    "                time.sleep(cooldown_remaining)\n",
    "                ready_proxies = [wait_proxy] \n",
    "\n",
    "        if not ready_proxies: \n",
    "            ready_proxies = available_proxies\n",
    "\n",
    "        def proxy_score(proxy):\n",
    "            if current_time - self.last_used.get(proxy, 0) >= COOLDOWN_TIME:\n",
    "                self.request_counts[proxy] = 0\n",
    "            \n",
    "            usage_factor = 1 / (self.request_counts[proxy] + 1)\n",
    "            success_factor = self.success_rates[proxy]\n",
    "            return usage_factor * success_factor\n",
    "\n",
    "        best_proxy = max(ready_proxies, key=proxy_score)\n",
    "        \n",
    "        self.request_counts[best_proxy] += 1\n",
    "        self.last_used[best_proxy] = current_time\n",
    "\n",
    "        return best_proxy\n",
    "\n",
    "    def mark_success(self, proxy):\n",
    "        self.success_rates[proxy] = min(1.0, self.success_rates[proxy] + 0.1)\n",
    "        if proxy in self.failed_proxies:\n",
    "            self.failed_proxies.remove(proxy)\n",
    "        self._update_monitoring()\n",
    "\n",
    "    def mark_failure(self, proxy):\n",
    "        self.success_rates[proxy] = max(0.0, self.success_rates[proxy] - 0.5) \n",
    "        if self.success_rates[proxy] <= 0.0:\n",
    "            self.failed_proxies.add(proxy)\n",
    "            logger.error(f\"üíÄ PROXY REMOVED: {proxy} due to repeated failures.\")\n",
    "        self._update_monitoring()\n",
    "\n",
    "    def _update_monitoring(self):\n",
    "        monitoring_data['proxy_stats'] = {\n",
    "            'total': len(self.proxies),\n",
    "            'working': len([p for p in self.proxies if p not in self.failed_proxies]),\n",
    "            'failed': len(self.failed_proxies),\n",
    "            'avg_success_rate': sum(self.success_rates.values()) / max(1, len(self.success_rates))\n",
    "        }\n",
    "\n",
    "def update_progress_display():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if monitoring_data['total_videos'] > 0:\n",
    "        progress_percent = (monitoring_data['processed_videos'] / monitoring_data['total_videos']) * 100\n",
    "        success_rate = (monitoring_data['successful_transcripts'] / max(1, monitoring_data['processed_videos'])) * 100\n",
    "    else:\n",
    "        progress_percent = 0\n",
    "        success_rate = 0\n",
    "\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * progress_percent / 100)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "\n",
    "    print(\"üéØ YOUTUBE TRANSCRIPT & METADATA EXTRACTION - LIVE MONITOR\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Progress: [{bar}] {progress_percent:.1f}%\")\n",
    "    print(f\"üìπ Videos: {monitoring_data['processed_videos']}/{monitoring_data['total_videos']}\")\n",
    "    print(f\"‚úÖ Success (Transcripts): {monitoring_data['successful_transcripts']} ({success_rate:.1f}%)\")\n",
    "    print(f\"‚ùå Failed (Transcripts): {monitoring_data['failed_videos']}\")\n",
    "    print(f\"üïê Runtime: {(time.time() - monitoring_data['start_time'])/60:.1f} minutes\")\n",
    "    print(f\"‚è±Ô∏è ETA: {monitoring_data['eta_minutes']:.1f} minutes remaining\")\n",
    "    print(f\"üì∫ Current: {monitoring_data['current_video']}\")\n",
    "    print(f\"üîÑ Status: {monitoring_data['current_status']}\")\n",
    "\n",
    "    if monitoring_data['proxy_stats']:\n",
    "        stats = monitoring_data['proxy_stats']\n",
    "        print(f\"üåê Proxies: {stats['working']}/{stats['total']} working (Success: {stats['avg_success_rate']:.2f}) - Cooldown: {COOLDOWN_TIME/60:.0f} mins\")\n",
    "\n",
    "    if sum(monitoring_data['transcript_types'].values()) > 0:\n",
    "        types = monitoring_data['transcript_types']\n",
    "        print(f\"üìù Types: Manual={types['manual']}, Auto={types['auto-generated']}, Unknown={types['unknown']}\")\n",
    "\n",
    "    if monitoring_data['recent_errors']:\n",
    "        print(f\"‚ö†Ô∏è Recent errors: {len(monitoring_data['recent_errors'])}\")\n",
    "        for error in monitoring_data['recent_errors'][-3:]:\n",
    "            print(f\"   ‚Ä¢ {error}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def fetch_video_metadata_requests(video_id, proxy):\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    \n",
    "    metadata = {\n",
    "        'title': None, 'channel_name': None, 'view_count': None, \n",
    "        'publish_date': None, 'video_length': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        session.proxies = {\"http\": proxy, \"https\": proxy}\n",
    "        headers = {'User-Agent': ua.random, 'Accept-Language': 'en-US,en;q=0.5'}\n",
    "        \n",
    "        response = session.get(url, headers=headers, timeout=REQUEST_TIMEOUT) \n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "\n",
    "        match = re.search(r'var ytInitialPlayerResponse\\s*=\\s*(\\{.*?\\})\\s*;', html_content)\n",
    "        if match:\n",
    "            data = json.loads(match.group(1))\n",
    "            \n",
    "            details = data.get('videoDetails', {})\n",
    "            metadata['title'] = details.get('title')\n",
    "            metadata['channel_name'] = details.get('author')\n",
    "            metadata['view_count'] = int(details.get('viewCount', 0)) if details.get('viewCount') else None\n",
    "            metadata['video_length'] = int(details.get('lengthSeconds', 0)) if details.get('lengthSeconds') else None\n",
    "\n",
    "            microformat = data.get('microformat', {}).get('playerMicroformatRenderer', {})\n",
    "            metadata['publish_date'] = microformat.get('publishDate')\n",
    "        else:\n",
    "            title_match = re.search(r'\"title\":\"([^\"]+)\"', html_content)\n",
    "            if title_match:\n",
    "                metadata['title'] = title_match.group(1)\n",
    "            \n",
    "            logger.warning(f\"‚ö†Ô∏è Metadata: Could not find full ytInitialPlayerResponse for {video_id}\")\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code in [404, 410]:\n",
    "             logger.warning(f\"‚ö†Ô∏è Metadata: Video is unavailable (404/410)\")\n",
    "             return metadata\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Metadata fetch error for {video_id}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def fetch_transcript_fixed(video_id, proxy):\n",
    "    try:\n",
    "        monitoring_data['current_status'] = f\"üîç Fetching transcript via {proxy[:20]}...\"\n",
    "        update_progress_display()\n",
    "\n",
    "        session = requests.Session()\n",
    "        session.proxies = {\"http\": proxy, \"https\": proxy}\n",
    "        headers = {\n",
    "            'User-Agent': ua.random, 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5', 'Connection': 'keep-alive', 'Referer': 'https://www.youtube.com/',\n",
    "        }\n",
    "\n",
    "        import youtube_transcript_api._api as _api\n",
    "        _api.requests = session\n",
    "\n",
    "        original_get = session.get\n",
    "        def enhanced_get(*args, **kwargs):\n",
    "            kwargs.setdefault('headers', {}).update(headers)\n",
    "            kwargs.setdefault('timeout', REQUEST_TIMEOUT) \n",
    "            return original_get(*args, **kwargs)\n",
    "        session.get = enhanced_get\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "        try:\n",
    "            transcript_list = ytt_api.list_transcripts(video_id)\n",
    "\n",
    "            try:\n",
    "                manual_transcript = transcript_list.find_manually_created_transcript(['en'])\n",
    "                fetched_transcript = manual_transcript.fetch()\n",
    "                transcript_text = \" \".join([snippet['text'] for snippet in fetched_transcript])\n",
    "                monitoring_data['transcript_types']['manual'] += 1\n",
    "                logger.info(\"‚úÖ Found MANUAL English transcript\")\n",
    "                return transcript_text, \"manual\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                auto_transcript = transcript_list.find_generated_transcript(['en'])\n",
    "                fetched_transcript = auto_transcript.fetch()\n",
    "                transcript_text = \" \".join([snippet['text'] for snippet in fetched_transcript])\n",
    "                monitoring_data['transcript_types']['auto-generated'] += 1\n",
    "                logger.info(\"‚úÖ Found AUTO-GENERATED English transcript\")\n",
    "                return transcript_text, \"auto-generated\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                fetched_transcript = ytt_api.fetch(video_id) \n",
    "                transcript_text = \" \".join([snippet['text'] for snippet in fetched_transcript])\n",
    "                monitoring_data['transcript_types']['unknown'] += 1\n",
    "                logger.info(\"‚úÖ Found transcript using fallback method\")\n",
    "                return transcript_text, \"unknown\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def get_video_details_with_retry(video_id, proxy_manager):\n",
    "    monitoring_data['current_status'] = f\"üé¨ Processing {video_id[:11]}...\"\n",
    "    update_progress_display()\n",
    "    \n",
    "    details = {\n",
    "        \"video_id\": video_id,\n",
    "        'title': None, 'channel_name': None, 'view_count': None, \n",
    "        'publish_date': None, 'video_length': None, \"transcript\": None,\n",
    "        \"transcript_type\": None, \"is_transcript_available\": False \n",
    "    }\n",
    "    \n",
    "    # === 1. Metadata Fetching Loop ===\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        proxy = proxy_manager.get_next_proxy()\n",
    "        proxy_display = proxy[:30] + \"...\" if len(proxy) > 30 else proxy\n",
    "        monitoring_data['current_status'] = f\"üîÑ Metadata Attempt {attempt}/{RETRIES} via {proxy_display}\"\n",
    "        update_progress_display()\n",
    "        logger.info(f\"üîÑ Metadata Attempt {attempt}/{RETRIES} with proxy: {proxy_display}\")\n",
    "\n",
    "        try:\n",
    "            metadata = fetch_video_metadata_requests(video_id, proxy)\n",
    "            proxy_manager.mark_success(proxy)\n",
    "            details.update(metadata)\n",
    "            logger.info(\"‚úÖ Metadata fetched successfully.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            proxy_manager.mark_failure(proxy)\n",
    "            logger.warning(f\"‚ö†Ô∏è Metadata failed: {error_msg[:100]}...\")\n",
    "            \n",
    "            if attempt < RETRIES:\n",
    "                delay = random.uniform(5, 15)\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    # === 2. Transcript Fetching Loop ===\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        proxy = proxy_manager.get_next_proxy()\n",
    "        proxy_display = proxy[:30] + \"...\" if len(proxy) > 30 else proxy\n",
    "        monitoring_data['current_status'] = f\"üîÑ Transcript Attempt {attempt}/{RETRIES} via {proxy_display}\"\n",
    "        update_progress_display()\n",
    "        logger.info(f\"üîÑ Transcript Attempt {attempt}/{RETRIES} with proxy: {proxy_display}\")\n",
    "\n",
    "        try:\n",
    "            transcript_text, transcript_type = fetch_transcript_fixed(video_id, proxy)\n",
    "\n",
    "            if transcript_text:\n",
    "                proxy_manager.mark_success(proxy)\n",
    "                monitoring_data['successful_transcripts'] += 1\n",
    "                monitoring_data['current_status'] = f\"‚úÖ Success ({transcript_type})\"\n",
    "                logger.info(f\"‚úÖ Transcript fetched successfully ({transcript_type})\")\n",
    "                details['transcript'] = transcript_text\n",
    "                details['transcript_type'] = transcript_type\n",
    "                details['is_transcript_available'] = True \n",
    "                return details\n",
    "            else:\n",
    "                proxy_manager.mark_success(proxy) \n",
    "                monitoring_data['current_status'] = \"‚ö†Ô∏è No transcript available\"\n",
    "                logger.warning(\"‚ö†Ô∏è No transcript available for this video\")\n",
    "                break \n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            short_error = error_msg[:50] + \"...\" if len(error_msg) > 50 else error_msg\n",
    "            monitoring_data['recent_errors'].append(f\"{video_id}: {short_error}\")\n",
    "            if len(monitoring_data['recent_errors']) > 10:\n",
    "                monitoring_data['recent_errors'] = monitoring_data['recent_errors'][-10:]\n",
    "\n",
    "            monitoring_data['current_status'] = f\"‚ùå Error: {short_error}\"\n",
    "            logger.warning(f\"‚ö†Ô∏è Transcript Failed: {error_msg[:100]}...\")\n",
    "\n",
    "            if any(phrase in error_msg.lower() for phrase in [\"blocking requests\", \"too many requests\", \"rate limit\", \"forbidden\", \"refused it\"]):\n",
    "                proxy_manager.mark_failure(proxy) \n",
    "\n",
    "            if attempt < RETRIES:\n",
    "                delay = random.uniform(15, 30) \n",
    "                time.sleep(delay)\n",
    "    \n",
    "    if not details['is_transcript_available']:\n",
    "        monitoring_data['failed_videos'] += 1\n",
    "        monitoring_data['current_status'] = \"‚ùå Final: No transcript found\"\n",
    "        logger.info(f\"‚ùå Final: No transcript for video {video_id}\")\n",
    "\n",
    "    return details\n",
    "\n",
    "def save_progress_report():\n",
    "    report_file = os.path.join(OUTPUT_FOLDER, \"progress_report.txt\") \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"YOUTUBE TRANSCRIPT EXTRACTION REPORT\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Total Videos: {monitoring_data['total_videos']}\\n\")\n",
    "        f.write(f\"Processed: {monitoring_data['processed_videos']}\\n\")\n",
    "        f.write(f\"Successful (Transcript): {monitoring_data['successful_transcripts']}\\n\")\n",
    "        f.write(f\"Failed (No Transcript): {monitoring_data['failed_videos']}\\n\")\n",
    "        f.write(f\"Success Rate (Transcript): {(monitoring_data['successful_transcripts']/max(1,monitoring_data['processed_videos']))*100:.1f}%\\n\")\n",
    "        f.write(f\"Runtime: {(time.time() - monitoring_data['start_time'])/60:.1f} minutes\\n\")\n",
    "        f.write(f\"\\nTranscript Types:\\n\")\n",
    "        for t_type, count in monitoring_data['transcript_types'].items():\n",
    "            f.write(f\"  {t_type}: {count}\\n\")\n",
    "        f.write(f\"\\nProxy Statistics:\\n\")\n",
    "        if monitoring_data['proxy_stats']:\n",
    "            stats = monitoring_data['proxy_stats']\n",
    "            f.write(f\"  Total: {stats['total']}\\n\")\n",
    "            f.write(f\"  Working: {stats['working']}\\n\")\n",
    "            f.write(f\"  Failed: {stats['failed']}\\n\")\n",
    "            f.write(f\"  Avg Success Rate: {stats['avg_success_rate']:.2f}\\n\")\n",
    "\n",
    "# MAIN EXECUTION WITH MONITORING\n",
    "monitoring_data['start_time'] = time.time()\n",
    "monitoring_data['current_status'] = \"üöÄ Starting extraction...\"\n",
    "update_progress_display()\n",
    "\n",
    "logger.info(\"üöÄ Starting YouTube details and transcript extraction with live monitoring...\")\n",
    "logger.info(f\"üìÅ Input CSV: {INPUT_CSV}\")\n",
    "logger.info(f\"üìÅ Output CSV: {OUTPUT_CSV}\")\n",
    "\n",
    "# Load input data\n",
    "try:\n",
    "    df_original = pd.read_csv(INPUT_CSV)\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"‚ùå Input file not found: {INPUT_CSV}\")\n",
    "    print(f\"ERROR: Please ensure the file is named '{os.path.basename(INPUT_CSV)}' and is in '{BASE_DIR}'\")\n",
    "    exit()\n",
    "\n",
    "print(f\"üìä Input CSV columns: {list(df_original.columns)}\")\n",
    "\n",
    "# Find video_id column (flexible column name detection, prioritizing 'Video_ID' based on your output)\n",
    "video_id_column = None\n",
    "for col in df_original.columns:\n",
    "    if col.lower() == 'video_id': # Exact match for your column name\n",
    "        video_id_column = col\n",
    "        break\n",
    "    if 'video_id' in col.lower() or 'videoid' in col.lower() or 'id' in col.lower():\n",
    "        video_id_column = col\n",
    "        break\n",
    "\n",
    "if video_id_column is None:\n",
    "    # If no video_id column found, check for URL and try to extract (omitted detailed URL logic for brevity, \n",
    "    # as the previous output shows you have a 'Video_ID' column, we assume this is the main path)\n",
    "    if not df_original.empty and len(df_original.columns) > 0:\n",
    "        video_id_column = df_original.columns[0]\n",
    "        logger.warning(f\"‚ö†Ô∏è No standard ID column found. Assuming the first column ('{video_id_column}') contains Video IDs.\")\n",
    "    else:\n",
    "        raise ValueError(\"No video identifier column found in CSV.\")\n",
    "\n",
    "# Temporarily rename the original ID column to 'video_id' for processing/merging consistency\n",
    "original_id_column_name = video_id_column \n",
    "if original_id_column_name != 'video_id':\n",
    "    df_original.rename(columns={original_id_column_name: 'video_id'}, inplace=True)\n",
    "\n",
    "video_ids = [str(x).strip() for x in df_original['video_id'].dropna().unique().tolist() if str(x).strip() != 'nan']\n",
    "monitoring_data['total_videos'] = len(video_ids)\n",
    "logger.info(f\"üìä Total unique videos found: {len(video_ids)}\")\n",
    "\n",
    "# Resume support\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    processed_df = pd.read_csv(OUTPUT_CSV)\n",
    "    \n",
    "    id_col_in_processed_df = original_id_column_name if original_id_column_name in processed_df.columns else 'video_id'\n",
    "    processed_ids = set(processed_df[id_col_in_processed_df].astype(str))\n",
    "    \n",
    "    if 'video_id' not in processed_df.columns and id_col_in_processed_df != 'video_id':\n",
    "         processed_df.rename(columns={id_col_in_processed_df: 'video_id'}, inplace=True)\n",
    "\n",
    "    scraped_cols_names = ['video_id', 'title', 'channel_name', 'view_count', 'publish_date', 'video_length', 'transcript', 'transcript_type', 'is_transcript_available']\n",
    "    results_cols = [col for col in processed_df.columns if col in scraped_cols_names]\n",
    "    results = processed_df[processed_df['video_id'].isin(video_ids)][results_cols].to_dict(\"records\")\n",
    "    \n",
    "    monitoring_data['successful_transcripts'] = processed_df['is_transcript_available'].sum()\n",
    "    monitoring_data['failed_videos'] = len(processed_df) - monitoring_data['successful_transcripts']\n",
    "    \n",
    "    logger.info(f\"‚ôªÔ∏è Resuming: {len(processed_ids)} already processed\")\n",
    "else:\n",
    "    processed_ids = set()\n",
    "    results = []\n",
    "\n",
    "remaining = [vid for vid in video_ids if vid not in processed_ids]\n",
    "logger.info(f\"‚è≥ Remaining videos to process: {len(remaining)}\")\n",
    "\n",
    "if len(remaining) == 0:\n",
    "    logger.info(\"üéâ All videos already processed! Bypassing processing loop.\")\n",
    "    # Exit here is the correct behavior if all are processed. \n",
    "    # The previous 0.0 minute run likely happened because len(remaining) was 0.\n",
    "    save_progress_report()\n",
    "    exit()\n",
    "\n",
    "# Initialize proxy manager with monitoring\n",
    "proxy_manager = MonitoringProxyManager(PROXIES)\n",
    "logger.info(f\"üåê Initialized with {len(PROXIES)} proxies\")\n",
    "\n",
    "# Main processing loop with enhanced monitoring\n",
    "processed_count = len(processed_ids)\n",
    "monitoring_data['processed_videos'] = processed_count\n",
    "\n",
    "try:\n",
    "    for idx, vid in enumerate(remaining, start=1):\n",
    "        monitoring_data['current_video'] = vid\n",
    "        monitoring_data['current_status'] = f\"üé¨ Starting {vid[:11]}...\"\n",
    "        update_progress_display()\n",
    "\n",
    "        logger.info(f\"\\nüìπ [{idx}/{len(remaining)}] Processing video: {vid}\")\n",
    "\n",
    "        video_details = get_video_details_with_retry(vid, proxy_manager)\n",
    "\n",
    "        results = [res for res in results if res.get('video_id') != vid]\n",
    "        results.append(video_details)\n",
    "\n",
    "        pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "        monitoring_data['current_status'] = \"üíæ Progress saved\"\n",
    "        update_progress_display()\n",
    "        logger.info(\"üíæ Progress saved to CSV\")\n",
    "\n",
    "        processed_count += 1\n",
    "        monitoring_data['processed_videos'] = processed_count\n",
    "\n",
    "        if processed_count > len(processed_ids):\n",
    "            elapsed_time = time.time() - monitoring_data['start_time']\n",
    "            videos_processed_this_session = processed_count - len(processed_ids)\n",
    "            avg_time_per_video = elapsed_time / videos_processed_this_session\n",
    "            remaining_videos = len(remaining) - idx\n",
    "            eta_seconds = remaining_videos * avg_time_per_video\n",
    "            monitoring_data['eta_minutes'] = eta_seconds / 60\n",
    "            logger.info(f\"‚è±Ô∏è ETA: {monitoring_data['eta_minutes']:.1f} minutes ({avg_time_per_video:.1f}s/video)\")\n",
    "\n",
    "        delay = random.uniform(MIN_DELAY, MAX_DELAY)\n",
    "\n",
    "        if (processed_count - len(processed_ids)) % 10 == 0:\n",
    "            extra_delay = random.uniform(180, 300) \n",
    "            monitoring_data['current_status'] = f\"üéØ Milestone break: {extra_delay:.1f}s\"\n",
    "            update_progress_display()\n",
    "            save_progress_report() \n",
    "            logger.info(f\"üéØ Milestone break: {extra_delay:.1f}s\")\n",
    "            time.sleep(extra_delay)\n",
    "\n",
    "        monitoring_data['current_status'] = f\"üò¥ Delay: {delay:.1f}s\"\n",
    "        update_progress_display()\n",
    "        logger.info(f\"üò¥ Standard delay: {delay:.1f}s\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"\\n‚èπÔ∏è Interrupted by user\")\n",
    "    monitoring_data['current_status'] = \"‚èπÔ∏è Interrupted by user\"\n",
    "    update_progress_display()\n",
    "\n",
    "# --- Final Merge and Cleanup (Ensures ALL original columns are kept) ---\n",
    "\n",
    "final_df_scraped = pd.DataFrame(results)\n",
    "\n",
    "# Remove any columns from the scraped data that conflict with the original structure columns before merge\n",
    "scraped_cols_to_keep = ['video_id', 'title', 'channel_name', 'view_count', 'publish_date', 'video_length', 'transcript', 'transcript_type', 'is_transcript_available']\n",
    "final_df_scraped = final_df_scraped[[col for col in scraped_cols_to_keep if col in final_df_scraped.columns]]\n",
    "\n",
    "# Perform LEFT MERGE: Keeps ALL columns from df_original.\n",
    "# Use distinct suffixes to ensure new columns are added.\n",
    "df_final = pd.merge(\n",
    "    df_original, \n",
    "    final_df_scraped, \n",
    "    on='video_id', \n",
    "    how='left',\n",
    "    suffixes=('_original', '_scraped')\n",
    ")\n",
    "\n",
    "# Rename the scraped columns to the final desired output names.\n",
    "df_final.rename(columns={\n",
    "    'title_scraped': 'video_title',\n",
    "    'channel_name_scraped': 'video_channel_name',\n",
    "    'view_count_scraped': 'video_view_count',\n",
    "    'publish_date_scraped': 'video_publish_date',\n",
    "    'video_length_scraped': 'video_length_seconds',\n",
    "    'transcript': 'video_transcript', \n",
    "    'transcript_type': 'transcript_source', \n",
    "    'is_transcript_available': 'is_transcript_available',\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean up duplicate columns resulting from the merge process\n",
    "cols_to_drop = [col for col in df_final.columns if col.endswith('_original') or col in ['title', 'channel_name', 'view_count', 'publish_date', 'video_length']]\n",
    "df_final.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Restore original video ID column name\n",
    "if original_id_column_name != 'video_id':\n",
    "     df_final.rename(columns={'video_id': original_id_column_name}, inplace=True) \n",
    "\n",
    "# Final save\n",
    "df_final.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Final statistics with monitoring update\n",
    "end_time = time.time()\n",
    "total_time = end_time - monitoring_data['start_time']\n",
    "\n",
    "monitoring_data['current_status'] = \"üéâ Completed!\"\n",
    "update_progress_display()\n",
    "\n",
    "logger.info(f\"\\nüéâ Process completed!\")\n",
    "logger.info(f\"‚è±Ô∏è Total runtime: {total_time/60:.1f} minutes\")\n",
    "logger.info(f\"üìä Videos processed: {processed_count}\")\n",
    "logger.info(f\"‚úÖ Successful transcripts: {monitoring_data['successful_transcripts']}\")\n",
    "logger.info(f\"‚ùå Videos with no transcripts: {monitoring_data['failed_videos']}\")\n",
    "logger.info(f\"üìÅ Results saved to: {OUTPUT_CSV}\")\n",
    "if processed_count > 0:\n",
    "    logger.info(f\"üìà Success rate: {monitoring_data['successful_transcripts']/processed_count*100:.1f}%\")\n",
    "\n",
    "# Save final report\n",
    "save_progress_report()\n",
    "\n",
    "# Display sample results\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    final_df = pd.read_csv(OUTPUT_CSV)\n",
    "    logger.info(f\"üìä Final CSV contains {len(final_df)} total videos\")\n",
    "    logger.info(f\"üìã Final CSV columns: {list(final_df.columns)}\") \n",
    "\n",
    "    if 'transcript_source' in final_df.columns:\n",
    "        type_counts = final_df['transcript_source'].value_counts()\n",
    "        logger.info(f\"üìã Transcript types: {dict(type_counts)}\")\n",
    "\n",
    "    logger.info(f\"üìù Sample results:\")\n",
    "    id_col_ref = original_id_column_name if original_id_column_name in final_df.columns else 'video_id'\n",
    "    for idx, row in final_df.head(3).iterrows():\n",
    "        title_preview = row.get('video_title', 'N/A')\n",
    "        title_preview = title_preview[:40] + \"...\" if isinstance(title_preview, str) and len(title_preview) > 40 else title_preview\n",
    "        transcript_preview = row.get('video_transcript', 'No Transcript')\n",
    "        preview = transcript_preview[:100] + \"...\" if isinstance(transcript_preview, str) and len(transcript_preview) > 100 else transcript_preview\n",
    "        transcript_available = row.get('is_transcript_available', False)\n",
    "        logger.info(f\"  Video ID: {row[id_col_ref]} | Title: {title_preview} | T-Available: {transcript_available} | Transcript Preview: {preview}\")\n",
    "\n",
    "print(f\"\\nüéØ Script completed successfully!\")\n",
    "print(f\"üìä Final Report saved to: {os.path.join(OUTPUT_FOLDER, 'progress_report.txt')}\") \n",
    "print(f\"üìã Progress Log saved to: {PROGRESS_LOG}\")\n",
    "\n",
    "# Create final summary visualization (optional)\n",
    "try:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    labels = ['Successful Transcript', 'No Transcript']\n",
    "    sizes = [monitoring_data['successful_transcripts'], monitoring_data['failed_videos']]\n",
    "    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Transcript Extraction Results')\n",
    "\n",
    "    if monitoring_data['transcript_types']:\n",
    "        types = monitoring_data['transcript_types']\n",
    "        labels2 = []\n",
    "        sizes2 = []\n",
    "        for t_type, count in types.items():\n",
    "            if count > 0:\n",
    "                labels2.append(t_type.title())\n",
    "                sizes2.append(count)\n",
    "\n",
    "        if sizes2:\n",
    "            ax2.pie(sizes2, labels=labels2, autopct='%1.1f%%', startangle=90)\n",
    "            ax2.set_title('Transcript Types')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    visualization_path = os.path.join(OUTPUT_FOLDER, 'results_summary.png')\n",
    "    plt.savefig(visualization_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"üìä Results visualization saved to: {visualization_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not create visualization: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ EXTRACTION COMPLETE - MONITORING DATA SAVED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
