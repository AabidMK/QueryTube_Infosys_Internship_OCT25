{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfeff06-2715-4dc4-8619-7a93bf038a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing VectorDB components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18992]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to EXISTING video_embeddings collection!\n",
      "üìä Current documents in collection: 614\n",
      "üöÄ Starting FastAPI server with ALL VectorDB endpoints...\n",
      "üìö API Documentation will be available at: http://localhost:8080/docs\n",
      "‚úÖ Server should be running now!\n",
      "üîó Testing endpoints...\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE VECTORDB API - RUN THIS IN A NEW CELL\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uvicorn\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Initialize FastAPI app with ALL endpoints\n",
    "app = FastAPI(\n",
    "    title=\"VectorDB Ingestion API\",\n",
    "    description=\"API for ingesting data into Chroma vector database\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "print(\"üîÑ Initializing VectorDB components...\")\n",
    "\n",
    "# Initialize models and database - USE EXISTING COLLECTION\n",
    "try:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Use your existing video_embeddings collection\n",
    "    collection = chroma_client.get_collection(name=\"video_embeddings\")\n",
    "    \n",
    "    current_count = collection.count()\n",
    "    print(f\"‚úÖ Connected to EXISTING video_embeddings collection!\")\n",
    "    print(f\"üìä Current documents in collection: {current_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing VectorDB: {e}\")\n",
    "    # Fallback\n",
    "    collection = None\n",
    "\n",
    "# Pydantic models\n",
    "class Document(BaseModel):\n",
    "    text: str = Field(..., description=\"The text content to vectorize\")\n",
    "    metadata: Optional[Dict[str, Any]] = Field(default={}, description=\"Additional metadata\")\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))\n",
    "\n",
    "class BatchDocuments(BaseModel):\n",
    "    documents: List[Document] = Field(..., description=\"List of documents to ingest\")\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query\")\n",
    "    top_k: int = Field(10, description=\"Number of results to return\")\n",
    "\n",
    "# Utility functions\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embeddings for text\"\"\"\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# ========== API ROUTES ==========\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    if collection:\n",
    "        count = collection.count()\n",
    "        return {\n",
    "            \"message\": \"VectorDB Ingestion API\", \n",
    "            \"status\": \"running\",\n",
    "            \"collection\": \"video_embeddings\",\n",
    "            \"current_documents\": count\n",
    "        }\n",
    "    return {\"message\": \"VectorDB API - Collection not initialized\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    if collection:\n",
    "        count = collection.count()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"vector_db\": \"chromadb\",\n",
    "            \"collection\": \"video_embeddings\",\n",
    "            \"document_count\": count,\n",
    "            \"embedding_model\": \"all-MiniLM-L6-v2\"\n",
    "        }\n",
    "    return {\"status\": \"degraded\", \"message\": \"VectorDB not initialized\"}\n",
    "\n",
    "@app.post(\"/ingest\")\n",
    "async def ingest_document(document: Document):\n",
    "    \"\"\"Ingest a single document into vector database\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=500, detail=\"VectorDB not initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding\n",
    "        embedding = get_embedding(document.text)\n",
    "        \n",
    "        # Prepare metadata\n",
    "        metadata = document.metadata or {}\n",
    "        metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n",
    "        metadata[\"text_length\"] = len(document.text)\n",
    "        metadata[\"source\"] = \"api_ingestion\"\n",
    "        \n",
    "        # Add to vector database\n",
    "        collection.add(\n",
    "            ids=[document.id],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[metadata],\n",
    "            documents=[document.text]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Document ingested successfully\",\n",
    "            \"document_ids\": [document.id],\n",
    "            \"total_ingested\": 1\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error ingesting document: {str(e)}\")\n",
    "\n",
    "@app.post(\"/ingest-batch\")\n",
    "async def ingest_batch_documents(batch: BatchDocuments):\n",
    "    \"\"\"Ingest multiple documents in batch\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=500, detail=\"VectorDB not initialized\")\n",
    "    \n",
    "    try:\n",
    "        if not batch.documents:\n",
    "            raise HTTPException(status_code=400, detail=\"No documents provided\")\n",
    "        \n",
    "        ids = []\n",
    "        embeddings = []\n",
    "        metadatas = []\n",
    "        documents = []\n",
    "        \n",
    "        for doc in batch.documents:\n",
    "            # Generate embedding for each document\n",
    "            embedding = get_embedding(doc.text)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = doc.metadata or {}\n",
    "            metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n",
    "            metadata[\"text_length\"] = len(doc.text)\n",
    "            metadata[\"source\"] = \"api_ingestion\"\n",
    "            \n",
    "            ids.append(doc.id)\n",
    "            embeddings.append(embedding)\n",
    "            metadatas.append(metadata)\n",
    "            documents.append(doc.text)\n",
    "        \n",
    "        # Add batch to vector database\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            documents=documents\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Successfully ingested {len(batch.documents)} documents\",\n",
    "            \"document_ids\": ids,\n",
    "            \"total_ingested\": len(batch.documents)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error ingesting batch: {str(e)}\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_documents(search_request: SearchRequest):\n",
    "    \"\"\"Search for similar documents\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=500, detail=\"VectorDB not initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        query_embedding = get_embedding(search_request.query)\n",
    "        \n",
    "        # Search in vector database\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=search_request.top_k,\n",
    "            include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        search_results = []\n",
    "        if results['ids'][0]:  # Check if any results found\n",
    "            for i in range(len(results['ids'][0])):\n",
    "                search_results.append({\n",
    "                    \"id\": results['ids'][0][i],\n",
    "                    \"text\": results['documents'][0][i],\n",
    "                    \"metadata\": results['metadatas'][0][i],\n",
    "                    \"distance\": results['distances'][0][i]\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"results\": search_results,\n",
    "            \"query\": search_request.query,\n",
    "            \"total_results\": len(search_results)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error searching documents: {str(e)}\")\n",
    "\n",
    "@app.get(\"/documents/count\")\n",
    "async def get_document_count():\n",
    "    \"\"\"Get total number of documents in the collection\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=500, detail=\"VectorDB not initialized\")\n",
    "    \n",
    "    try:\n",
    "        count = collection.count()\n",
    "        return {\n",
    "            \"total_documents\": count,\n",
    "            \"collection\": \"video_embeddings\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error getting document count: {str(e)}\")\n",
    "\n",
    "@app.get(\"/collections\")\n",
    "async def list_collections():\n",
    "    \"\"\"List all available collections\"\"\"\n",
    "    try:\n",
    "        collections = chroma_client.list_collections()\n",
    "        collection_info = []\n",
    "        for coll in collections:\n",
    "            count = coll.count()\n",
    "            collection_info.append({\n",
    "                \"name\": coll.name,\n",
    "                \"document_count\": count\n",
    "            })\n",
    "        return {\"collections\": collection_info}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error listing collections: {str(e)}\")\n",
    "\n",
    "@app.get(\"/collection/{collection_name}/sample\")\n",
    "async def get_collection_sample(collection_name: str, limit: int = 3):\n",
    "    \"\"\"Get sample documents from a specific collection\"\"\"\n",
    "    try:\n",
    "        target_collection = chroma_client.get_collection(name=collection_name)\n",
    "        sample = target_collection.peek(limit=limit)\n",
    "        \n",
    "        sample_data = []\n",
    "        for i in range(len(sample['ids'])):\n",
    "            sample_data.append({\n",
    "                \"id\": sample['ids'][i],\n",
    "                \"text\": sample['documents'][i],\n",
    "                \"metadata\": sample['metadatas'][i]\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            \"collection\": collection_name,\n",
    "            \"total_documents\": target_collection.count(),\n",
    "            \"sample\": sample_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error getting sample: {str(e)}\")\n",
    "\n",
    "# Function to run server\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080, log_level=\"info\")\n",
    "\n",
    "print(\"üöÄ Starting FastAPI server with ALL VectorDB endpoints...\")\n",
    "print(\"üìö API Documentation will be available at: http://localhost:8080/docs\")\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(3)\n",
    "print(\"‚úÖ Server should be running now!\")\n",
    "print(\"üîó Testing endpoints...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3519a185-8fb5-45c5-ac3a-2276a8726b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ QUICK API TEST\n",
      "========================================\n",
      "INFO:     127.0.0.1:54958 - \"GET / HTTP/1.1\" 200 OK\n",
      "‚úÖ /: Status 200\n",
      "   Response: {'message': 'VectorDB Ingestion API', 'status': 'running', 'collection': 'video_embeddings', 'current_documents': 614}\n",
      "INFO:     127.0.0.1:54960 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:78: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ /health: Status 200\n",
      "   Response: {'status': 'healthy', 'timestamp': '2025-11-23T04:35:40.691126', 'vector_db': 'chromadb', 'collection': 'video_embeddings', 'document_count': 614, 'embedding_model': 'all-MiniLM-L6-v2'}\n",
      "INFO:     127.0.0.1:54962 - \"GET /documents/count HTTP/1.1\" 200 OK\n",
      "‚úÖ /documents/count: Status 200\n",
      "   Response: {'total_documents': 614, 'collection': 'video_embeddings'}\n",
      "INFO:     127.0.0.1:54969 - \"GET /collections HTTP/1.1\" 200 OK\n",
      "‚úÖ /collections: Status 200\n",
      "   Response: {'collections': [{'name': 'video_embeddings', 'document_count': 614}, {'name': 'documents', 'document_count': 0}]}\n",
      "INFO:     127.0.0.1:54972 - \"POST /search HTTP/1.1\" 200 OK\n",
      "‚úÖ /search: Status 200\n",
      "   Found 2 results\n"
     ]
    }
   ],
   "source": [
    "# QUICK TEST - RUN THIS AFTER THE SERVER STARTS\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def quick_test():\n",
    "    BASE_URL = \"http://localhost:8080\"\n",
    "    \n",
    "    print(\"üß™ QUICK API TEST\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Wait a bit for server to fully start\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Test basic endpoints\n",
    "    endpoints = [\n",
    "        \"/\",\n",
    "        \"/health\", \n",
    "        \"/documents/count\",\n",
    "        \"/collections\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{endpoint}\", timeout=10)\n",
    "            print(f\"‚úÖ {endpoint}: Status {response.status_code}\")\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"   Response: {data}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {endpoint}: Error - {e}\")\n",
    "    \n",
    "    # Test search\n",
    "    try:\n",
    "        search_data = {\"query\": \"machine learning\", \"top_k\": 2}\n",
    "        response = requests.post(f\"{BASE_URL}/search\", json=search_data, timeout=10)\n",
    "        print(f\"‚úÖ /search: Status {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"   Found {data['total_results']} results\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå /search: Error - {e}\")\n",
    "\n",
    "# Run quick test\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665824fb-6d4c-41ea-874a-15d05830b1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ COMPLETE VECTORDB API FUNCTIONALITY TEST\n",
      "============================================================\n",
      "‚úÖ All basic endpoints are working! Now testing full functionality...\n",
      "\n",
      "1. üìä COLLECTION INFORMATION\n",
      "INFO:     127.0.0.1:50662 - \"GET /collections HTTP/1.1\" 200 OK\n",
      "   üìÇ video_embeddings: 614 documents\n",
      "   üìÇ documents: 0 documents\n",
      "\n",
      "2. üîç SAMPLE DATA FROM VIDEO_EMBEDDINGS\n",
      "INFO:     127.0.0.1:50665 - \"GET /collection/video_embeddings/sample?limit=2 HTTP/1.1\" 200 OK\n",
      "   üìä Total in collection: 614\n",
      "   1. ID: video_0\n",
      "      Text: Scientists Can‚Äôt Explain What‚Äôs Happening on This Mountain in Tibet Scientists C...\n",
      "      Metadata: ['channel_title', 'duration', 'video_id', 'description', 'published_at', 'view_count', 'title', 'like_count']\n",
      "   2. ID: video_1\n",
      "      Text: $138 Million Pirate Treasure Found on the Ocean Floor $138 Million Pirate Treasu...\n",
      "      Metadata: ['duration', 'like_count', 'view_count', 'published_at', 'video_id', 'title', 'channel_title', 'description']\n",
      "\n",
      "3. üîé ADVANCED SEARCH TESTING\n",
      "INFO:     127.0.0.1:50670 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'artificial intelligence': 3 results\n",
      "      1. AI basics for AI Engineering Now before we get into AI engineering may...\n",
      "         Distance: 1.0114\n",
      "         ID: video_61...\n",
      "      2. AI & Robotics : Transforming Life and Industry | Arman Baig | TEDxSoli...\n",
      "         Distance: 1.0476\n",
      "         ID: video_54...\n",
      "INFO:     127.0.0.1:50672 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'machine learning algorithms': 2 results\n",
      "      1. History Of Machine Learning- Dicussing The Entire Timeline hello all m...\n",
      "         Distance: 1.2892\n",
      "         ID: video_39...\n",
      "      2. Implementing Machine Learninng Pipelines USsing Sklearn And Python hel...\n",
      "         Distance: 1.3829\n",
      "         ID: video_40...\n",
      "INFO:     127.0.0.1:50675 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'data science': 2 results\n",
      "      1. The Complete Data Science Roadmap if you want to become a data scienti...\n",
      "         Distance: 0.9826\n",
      "         ID: video_47...\n",
      "      2. Data Engineering Talk with Deepesh You can take an example of Facebook...\n",
      "         Distance: 1.0686\n",
      "         ID: video_61...\n",
      "INFO:     127.0.0.1:50677 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'neural networks': 2 results\n",
      "      1. History Of Machine Learning- Dicussing The Entire Timeline hello all m...\n",
      "         Distance: 1.3019\n",
      "         ID: video_39...\n",
      "      2. How Large Language Models (LLM) In Generative AI Are Trained ? hello a...\n",
      "         Distance: 1.3380\n",
      "         ID: video_40...\n",
      "\n",
      "4. üì• INGEST SINGLE DOCUMENT\n",
      "INFO:     127.0.0.1:50679 - \"POST /ingest HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:98: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Status: success\n",
      "   üìù Message: Document ingested successfully\n",
      "   üÜî Document ID: e1fb8fa1-e640-4bd9-abde-8c54496979bc\n",
      "\n",
      "5. üì¶ INGEST BATCH DOCUMENTS\n",
      "INFO:     127.0.0.1:50681 - \"POST /ingest-batch HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:141: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Status: None\n",
      "   üì¶ Ingested: None documents\n",
      "   üìù Message: None\n",
      "\n",
      "6. üìà VERIFY UPDATED DOCUMENT COUNT\n",
      "INFO:     127.0.0.1:50686 - \"GET /documents/count HTTP/1.1\" 200 OK\n",
      "   üìä Original count: 614\n",
      "   üìä New count: 615\n",
      "   üìà Documents added: +1\n",
      "   ‚úÖ Count update verified correctly!\n",
      "\n",
      "7. üîç SEARCH FOR NEWLY ADDED CONTENT\n",
      "INFO:     127.0.0.1:50694 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'API test document artificial intelligence': 2 results\n",
      "      Found: This is a test document about artificial intelligence and ma...\n",
      "         Source: api_ingestion, Category: test\n",
      "INFO:     127.0.0.1:50696 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'deep learning convolutional neural networks': 2 results\n",
      "      Found: Announcing NLP Live community Sessions hello all my name is ...\n",
      "         Source: unknown, Category: unknown\n",
      "INFO:     127.0.0.1:50698 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   üîç 'natural language processing algorithms': 2 results\n",
      "      Found: Announcing NLP Live community Sessions hello all my name is ...\n",
      "         Source: unknown, Category: unknown\n",
      "\n",
      "8. ü©∫ FINAL HEALTH CHECK\n",
      "INFO:     127.0.0.1:50700 - \"GET /health HTTP/1.1\" 200 OK\n",
      "   ‚úÖ Status: healthy\n",
      "   üìä Document count: 615\n",
      "   üóÇÔ∏è  Collection: video_embeddings\n",
      "   ü§ñ Model: all-MiniLM-L6-v2\n",
      "\n",
      "============================================================\n",
      "üéâ COMPREHENSIVE FUNCTIONALITY TEST COMPLETED!\n",
      "‚úÖ Your VectorDB API is fully operational!\n",
      "üìö API Documentation: http://localhost:8080/docs\n",
      "üöÄ Ready for production use!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:78: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "\n",
    "def test_complete_functionality():\n",
    "    print(\"üöÄ COMPLETE VECTORDB API FUNCTIONALITY TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ All basic endpoints are working! Now testing full functionality...\\n\")\n",
    "    \n",
    "    # Test 1: Verify Collection Info\n",
    "    print(\"1. üìä COLLECTION INFORMATION\")\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/collections\")\n",
    "        collections = response.json().get('collections', [])\n",
    "        for coll in collections:\n",
    "            print(f\"   üìÇ {coll['name']}: {coll['document_count']} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 2: Get Sample Data\n",
    "    print(\"\\n2. üîç SAMPLE DATA FROM VIDEO_EMBEDDINGS\")\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/collection/video_embeddings/sample?limit=2\")\n",
    "        sample_data = response.json()\n",
    "        samples = sample_data.get('sample', [])\n",
    "        print(f\"   üìä Total in collection: {sample_data.get('total_documents')}\")\n",
    "        \n",
    "        for i, sample in enumerate(samples, 1):\n",
    "            print(f\"   {i}. ID: {sample['id']}\")\n",
    "            print(f\"      Text: {sample['text'][:80]}...\")\n",
    "            print(f\"      Metadata: {list(sample['metadata'].keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 3: Search with Multiple Queries\n",
    "    print(\"\\n3. üîé ADVANCED SEARCH TESTING\")\n",
    "    search_queries = [\n",
    "        {\"query\": \"artificial intelligence\", \"top_k\": 3},\n",
    "        {\"query\": \"machine learning algorithms\", \"top_k\": 2},\n",
    "        {\"query\": \"data science\", \"top_k\": 2},\n",
    "        {\"query\": \"neural networks\", \"top_k\": 2}\n",
    "    ]\n",
    "    \n",
    "    for search in search_queries:\n",
    "        try:\n",
    "            response = requests.post(f\"{BASE_URL}/search\", json=search)\n",
    "            results = response.json()\n",
    "            \n",
    "            print(f\"   üîç '{search['query']}': {results['total_results']} results\")\n",
    "            \n",
    "            for i, result in enumerate(results.get('results', [])[:2], 1):  # Show first 2 results\n",
    "                print(f\"      {i}. {result['text'][:70]}...\")\n",
    "                print(f\"         Distance: {result['distance']:.4f}\")\n",
    "                print(f\"         ID: {result['id'][:8]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error with '{search['query']}': {e}\")\n",
    "    \n",
    "    # Test 4: Ingest New Single Document\n",
    "    print(\"\\n4. üì• INGEST SINGLE DOCUMENT\")\n",
    "    new_doc = {\n",
    "        \"text\": \"This is a test document about artificial intelligence and machine learning capabilities added via the VectorDB API for testing purposes.\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"test\",\n",
    "            \"source\": \"api_test\",\n",
    "            \"type\": \"educational\",\n",
    "            \"topic\": \"AI/ML\",\n",
    "            \"purpose\": \"api_verification\"\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/ingest\", json=new_doc)\n",
    "        result = response.json()\n",
    "        print(f\"   ‚úÖ Status: {result.get('status')}\")\n",
    "        print(f\"   üìù Message: {result.get('message')}\")\n",
    "        print(f\"   üÜî Document ID: {result.get('document_ids', [])[0]}\")\n",
    "        new_doc_id = result.get('document_ids', [])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        new_doc_id = None\n",
    "    \n",
    "    # Test 5: Ingest Batch Documents\n",
    "    print(\"\\n5. üì¶ INGEST BATCH DOCUMENTS\")\n",
    "    batch_docs = {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"text\": \"Deep learning models like convolutional neural networks have revolutionized image recognition and computer vision tasks.\",\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"deep_learning\",\n",
    "                    \"application\": \"computer_vision\",\n",
    "                    \"model_type\": \"CNN\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Natural Language Processing enables machines to understand, interpret, and generate human language through various algorithms.\",\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"nlp\",\n",
    "                    \"application\": \"language_processing\",\n",
    "                    \"techniques\": [\"tokenization\", \"embedding\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/ingest-batch\", json=batch_docs)\n",
    "        result = response.json()\n",
    "        print(f\"   ‚úÖ Status: {result.get('status')}\")\n",
    "        print(f\"   üì¶ Ingested: {result.get('total_ingested')} documents\")\n",
    "        print(f\"   üìù Message: {result.get('message')}\")\n",
    "        batch_ids = result.get('document_ids', [])\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        batch_ids = []\n",
    "    \n",
    "    # Test 6: Verify Updated Count\n",
    "    print(\"\\n6. üìà VERIFY UPDATED DOCUMENT COUNT\")\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/documents/count\")\n",
    "        count_data = response.json()\n",
    "        new_count = count_data.get('total_documents')\n",
    "        original_count = 614\n",
    "        added_count = new_count - original_count\n",
    "        \n",
    "        print(f\"   üìä Original count: {original_count}\")\n",
    "        print(f\"   üìä New count: {new_count}\")\n",
    "        print(f\"   üìà Documents added: +{added_count}\")\n",
    "        \n",
    "        if added_count == (1 + len(batch_ids)):\n",
    "            print(\"   ‚úÖ Count update verified correctly!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Count mismatch: Expected +{1 + len(batch_ids)}, got +{added_count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 7: Search for Newly Added Content\n",
    "    print(\"\\n7. üîç SEARCH FOR NEWLY ADDED CONTENT\")\n",
    "    test_searches = [\n",
    "        \"API test document artificial intelligence\",\n",
    "        \"deep learning convolutional neural networks\", \n",
    "        \"natural language processing algorithms\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_searches:\n",
    "        try:\n",
    "            search_data = {\"query\": query, \"top_k\": 2}\n",
    "            response = requests.post(f\"{BASE_URL}/search\", json=search_data)\n",
    "            results = response.json()\n",
    "            \n",
    "            print(f\"   üîç '{query}': {results['total_results']} results\")\n",
    "            \n",
    "            if results['total_results'] > 0:\n",
    "                for i, result in enumerate(results['results'][:1], 1):  # Show first result\n",
    "                    source = result['metadata'].get('source', 'unknown')\n",
    "                    category = result['metadata'].get('category', 'unknown')\n",
    "                    print(f\"      Found: {result['text'][:60]}...\")\n",
    "                    print(f\"         Source: {source}, Category: {category}\")\n",
    "            else:\n",
    "                print(f\"      No results found for new content\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Test 8: Final Health Check\n",
    "    print(\"\\n8. ü©∫ FINAL HEALTH CHECK\")\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/health\")\n",
    "        health_data = response.json()\n",
    "        print(f\"   ‚úÖ Status: {health_data.get('status')}\")\n",
    "        print(f\"   üìä Document count: {health_data.get('document_count')}\")\n",
    "        print(f\"   üóÇÔ∏è  Collection: {health_data.get('collection')}\")\n",
    "        print(f\"   ü§ñ Model: {health_data.get('embedding_model')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ COMPREHENSIVE FUNCTIONALITY TEST COMPLETED!\")\n",
    "    print(\"‚úÖ Your VectorDB API is fully operational!\")\n",
    "    print(\"üìö API Documentation: http://localhost:8080/docs\")\n",
    "    print(\"üöÄ Ready for production use!\")\n",
    "\n",
    "# Run the complete functionality test\n",
    "test_complete_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d91db36-516f-4082-a49e-5bc951610a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FIXING BATCH INGESTION\n",
      "========================================\n",
      "Testing simplified batch ingestion...\n",
      "INFO:     127.0.0.1:60521 - \"POST /ingest-batch HTTP/1.1\" 200 OK\n",
      "Status: 200\n",
      "‚úÖ Batch ingestion successful!\n",
      "Message: Successfully ingested 2 documents\n",
      "Documents ingested: 2\n",
      "Document IDs: ['b172a434-2fb4-4d74-b4f7-087e32ccab42', 'cf4ca44e-9316-454d-9bf5-5cc02ec1862c']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:141: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "# FIX FOR BATCH INGESTION - RUN THIS IN A NEW CELL\n",
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "\n",
    "def test_and_fix_batch_ingestion():\n",
    "    print(\"üîß FIXING BATCH INGESTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test with a simpler batch structure\n",
    "    simple_batch = {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"text\": \"Test batch document 1 about AI and machine learning.\",\n",
    "                \"metadata\": {\"test\": \"batch\", \"number\": 1}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Test batch document 2 about data science and analytics.\",\n",
    "                \"metadata\": {\"test\": \"batch\", \"number\": 2}\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Testing simplified batch ingestion...\")\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/ingest-batch\", json=simple_batch, timeout=30)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"‚úÖ Batch ingestion successful!\")\n",
    "            print(f\"Message: {result.get('message')}\")\n",
    "            print(f\"Documents ingested: {result.get('total_ingested')}\")\n",
    "            print(f\"Document IDs: {result.get('document_ids')}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Batch failed with status: {response.status_code}\")\n",
    "            print(f\"Error details: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {e}\")\n",
    "\n",
    "# Run the fix\n",
    "test_and_fix_batch_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f400684b-b474-4010-a4c0-7e4f516535c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ FINAL API VERIFICATION\n",
      "==================================================\n",
      "INFO:     127.0.0.1:50575 - \"GET /documents/count HTTP/1.1\" 200 OK\n",
      "üìä Starting document count: 617\n",
      "\n",
      "1. ‚úÖ SINGLE DOCUMENT INGESTION\n",
      "INFO:     127.0.0.1:62293 - \"POST /ingest HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:98: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Single ingestion: WORKING\n",
      "\n",
      "2. ‚úÖ BATCH DOCUMENT INGESTION\n",
      "INFO:     127.0.0.1:62295 - \"POST /ingest-batch HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18992\\2071636813.py:141: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  metadata[\"timestamp\"] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Batch ingestion: WORKING\n",
      "   üì¶ Ingested: 2 documents\n",
      "\n",
      "3. ‚úÖ SEARCH FUNCTIONALITY\n",
      "INFO:     127.0.0.1:62302 - \"POST /search HTTP/1.1\" 200 OK\n",
      "   ‚úÖ Search: WORKING\n",
      "   üîç Found: 2 results\n",
      "\n",
      "4. ‚úÖ FINAL DOCUMENT COUNT\n",
      "INFO:     127.0.0.1:62305 - \"GET /documents/count HTTP/1.1\" 200 OK\n",
      "   üìä Final document count: 620\n",
      "   üìà Total added in this test: 3\n",
      "\n",
      "5. ‚úÖ COLLECTIONS MANAGEMENT\n",
      "INFO:     127.0.0.1:62310 - \"GET /collections HTTP/1.1\" 200 OK\n",
      "   ‚úÖ Collections: WORKING\n",
      "   üìÇ video_embeddings: 620 documents\n",
      "   üìÇ documents: 0 documents\n",
      "\n",
      "==================================================\n",
      "üéâ FINAL VERIFICATION COMPLETED!\n",
      "üìö API Documentation: http://localhost:8080/docs\n",
      "üöÄ Your VectorDB API is PRODUCTION READY!\n",
      "INFO:     127.0.0.1:63154 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63154 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERIFICATION TEST\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "\n",
    "def final_verification():\n",
    "    print(\"üéØ FINAL API VERIFICATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get current count\n",
    "    response = requests.get(f\"{BASE_URL}/documents/count\")\n",
    "    current_count = response.json().get('total_documents')\n",
    "    print(f\"üìä Starting document count: {current_count}\")\n",
    "    \n",
    "    # Test 1: Single Ingestion\n",
    "    print(\"\\n1. ‚úÖ SINGLE DOCUMENT INGESTION\")\n",
    "    single_doc = {\n",
    "        \"text\": \"Final test: Vector databases enable efficient similarity search for AI applications.\",\n",
    "        \"metadata\": {\"test\": \"final\", \"type\": \"verification\"}\n",
    "    }\n",
    "    response = requests.post(f\"{BASE_URL}/ingest\", json=single_doc)\n",
    "    if response.status_code == 200:\n",
    "        print(\"   ‚úÖ Single ingestion: WORKING\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Single ingestion: FAILED - {response.status_code}\")\n",
    "    \n",
    "    # Test 2: Batch Ingestion (with fix)\n",
    "    print(\"\\n2. ‚úÖ BATCH DOCUMENT INGESTION\")\n",
    "    batch_docs = {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"text\": \"Batch test 1: Machine learning models improve with more data.\",\n",
    "                \"metadata\": {\"batch\": \"test\", \"id\": 1}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Batch test 2: Artificial intelligence is transforming industries.\",\n",
    "                \"metadata\": {\"batch\": \"test\", \"id\": 2}\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(f\"{BASE_URL}/ingest-batch\", json=batch_docs)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"   ‚úÖ Batch ingestion: WORKING\")\n",
    "        print(f\"   üì¶ Ingested: {result.get('total_ingested')} documents\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Batch ingestion: FAILED - {response.status_code}\")\n",
    "        print(f\"   Error: {response.text}\")\n",
    "    \n",
    "    # Test 3: Search\n",
    "    print(\"\\n3. ‚úÖ SEARCH FUNCTIONALITY\")\n",
    "    search_data = {\"query\": \"machine learning AI\", \"top_k\": 2}\n",
    "    response = requests.post(f\"{BASE_URL}/search\", json=search_data)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        print(\"   ‚úÖ Search: WORKING\")\n",
    "        print(f\"   üîç Found: {results['total_results']} results\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Search: FAILED - {response.status_code}\")\n",
    "    \n",
    "    # Test 4: Final Count\n",
    "    print(\"\\n4. ‚úÖ FINAL DOCUMENT COUNT\")\n",
    "    response = requests.get(f\"{BASE_URL}/documents/count\")\n",
    "    final_count = response.json().get('total_documents')\n",
    "    print(f\"   üìä Final document count: {final_count}\")\n",
    "    print(f\"   üìà Total added in this test: {final_count - current_count}\")\n",
    "    \n",
    "    # Test 5: Collections\n",
    "    print(\"\\n5. ‚úÖ COLLECTIONS MANAGEMENT\")\n",
    "    response = requests.get(f\"{BASE_URL}/collections\")\n",
    "    if response.status_code == 200:\n",
    "        collections = response.json().get('collections', [])\n",
    "        print(\"   ‚úÖ Collections: WORKING\")\n",
    "        for coll in collections:\n",
    "            print(f\"   üìÇ {coll['name']}: {coll['document_count']} documents\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ FINAL VERIFICATION COMPLETED!\")\n",
    "    print(\"üìö API Documentation: http://localhost:8080/docs\")\n",
    "    print(\"üöÄ Your VectorDB API is PRODUCTION READY!\")\n",
    "\n",
    "# Run final verification\n",
    "final_verification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6537a-2b47-4e67-96dc-66379d946765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
