{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install isodate"
      ],
      "metadata": {
        "id": "CU_8uI1YRDWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj1QF0Z8Q_PL"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import isodate\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = \"your Youtube API KEY\"\n",
        "CHANNEL_ID = \"your Channel id\"  # Example: Google Developers\n",
        "\n",
        "# Build YouTube API service\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Get channel details\n",
        "channel_response = youtube.channels().list(\n",
        "    part=\"snippet,contentDetails,statistics\",\n",
        "    id=CHANNEL_ID\n",
        ").execute()\n",
        "\n",
        "channel_data = channel_response[\"items\"][0]\n",
        "channel_title = channel_data[\"snippet\"][\"title\"]\n",
        "channel_description = channel_data[\"snippet\"][\"description\"]\n",
        "channel_country = channel_data[\"snippet\"].get(\"country\", \"N/A\")\n",
        "channel_thumbnail = channel_data[\"snippet\"][\"thumbnails\"][\"high\"][\"url\"]\n",
        "channel_subscribers = channel_data[\"statistics\"].get(\"subscriberCount\", \"0\")\n",
        "channel_video_count = channel_data[\"statistics\"].get(\"videoCount\", \"0\")\n",
        "\n",
        "# Get Uploads playlist ID\n",
        "uploads_id = channel_data[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
        "\n",
        "# Get videos from uploads playlist\n",
        "videos = youtube.playlistItems().list(\n",
        "    part=\"snippet,contentDetails\",\n",
        "    playlistId=uploads_id,\n",
        "    maxResults=50  # fetch latest 50 videos\n",
        ").execute()\n",
        "\n",
        "video_list = []\n",
        "print(\"üé• Latest Videos:\\n\")\n",
        "\n",
        "for item in videos[\"items\"]:\n",
        "    vid_id = item[\"contentDetails\"][\"videoId\"]\n",
        "\n",
        "    # Get video details\n",
        "    video = youtube.videos().list(\n",
        "        part=\"snippet,contentDetails,statistics,status\",\n",
        "        id=vid_id\n",
        "    ).execute()\n",
        "\n",
        "    if not video[\"items\"]:\n",
        "        continue\n",
        "\n",
        "    data = video[\"items\"][0]\n",
        "\n",
        "    title = data[\"snippet\"][\"title\"]\n",
        "    desc = data[\"snippet\"][\"description\"][:100] + \"...\"\n",
        "    published_at = data[\"snippet\"][\"publishedAt\"]\n",
        "    category_id = data[\"snippet\"][\"categoryId\"]\n",
        "    default_language = data[\"snippet\"].get(\"defaultLanguage\", \"N/A\")\n",
        "    duration = isodate.parse_duration(data[\"contentDetails\"][\"duration\"]).total_seconds()\n",
        "    thumbnails = data[\"snippet\"][\"thumbnails\"][\"high\"][\"url\"]\n",
        "    views = data[\"statistics\"].get(\"viewCount\", \"0\")\n",
        "    likes = data[\"statistics\"].get(\"likeCount\", \"0\")\n",
        "    comments = data[\"statistics\"].get(\"commentCount\", \"0\")\n",
        "    privacy_status = data.get(\"status\", {}).get(\"privacyStatus\", \"N/A\")\n",
        "\n",
        "    print(f\"Video ID: {vid_id}\")\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Published At: {published_at}\")\n",
        "    print(f\"Views: {views}\")\n",
        "    print(f\"Likes: {likes}\")\n",
        "    print(f\"Comments: {comments}\\n\")\n",
        "\n",
        "    video_list.append({\n",
        "        \"Video ID\": vid_id,\n",
        "        \"Title\": title,\n",
        "        \"Description\": desc,\n",
        "        \"Published At\": published_at,\n",
        "        \"Category ID\": category_id,\n",
        "        \"Default Language\": default_language,\n",
        "        \"Thumbnail (High)\": thumbnails,\n",
        "        \"Duration (seconds)\": duration,\n",
        "        \"View Count\": views,\n",
        "        \"Like Count\": likes,\n",
        "        \"Comment Count\": comments,\n",
        "        \"Privacy Status\": privacy_status,\n",
        "        \"Channel ID\": CHANNEL_ID,\n",
        "        \"Channel Title\": channel_title,\n",
        "        \"Channel Description\": channel_description,\n",
        "        \"Channel Country\": channel_country,\n",
        "        \"Channel Thumbnail\": channel_thumbnail,\n",
        "        \"Channel Subscribers\": channel_subscribers,\n",
        "        \"Channel Video Count\": channel_video_count\n",
        "    })\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(video_list)\n",
        "df.to_csv(\"youtube_videos_with_channel.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Data saved to youtube_videos_with_channel.csv successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import concurrent.futures\n",
        "\n",
        "API_KEY = \"your search api key\"\n",
        "INPUT_CSV = \" your file path\"\n",
        "OUTPUT_CSV = \"output_transcripts.csv\"\n",
        "\n",
        "def fetch_transcript(video_id):\n",
        "    \"\"\"Fetch transcript for one video\"\"\"\n",
        "    url = \"https://www.searchapi.io/api/v1/search\"\n",
        "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
        "    params = {\n",
        "        \"engine\": \"youtube_transcripts\",\n",
        "        \"video_id\": video_id,\n",
        "        \"lang\": \"en\",\n",
        "        \"transcript_type\": \"auto\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        if response.status_code != 200:\n",
        "            return video_id, f\"‚ùå Error {response.status_code}\"\n",
        "        data = response.json()\n",
        "\n",
        "        if \"transcripts\" not in data or not data[\"transcripts\"]:\n",
        "            return video_id, \"‚ö† No transcript available\"\n",
        "\n",
        "        transcript_text = \" \".join(seg[\"text\"] for seg in data[\"transcripts\"])\n",
        "        return video_id, transcript_text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return video_id, f\"üö® Error: {e}\"\n",
        "\n",
        "def process_videos():\n",
        "    results = []\n",
        "    with open(INPUT_CSV, \"r\", encoding=\"utf-8\") as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        video_ids = [row[\"id\"].strip() for row in reader]\n",
        "\n",
        "    print(f\"üöÄ Fetching transcripts for {len(video_ids)} videos in parallel...\")\n",
        "\n",
        "    # Use up to 10 threads for speed\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(fetch_transcript, vid): vid for vid in video_ids}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            video_id, transcript = future.result()\n",
        "            results.append({\"video_id\": video_id, \"transcript\": transcript})\n",
        "            print(f\"‚úÖ {video_id} processed\")\n",
        "\n",
        "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=[\"video_id\", \"transcript\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"\\nüéâ All transcripts saved successfully to:\", OUTPUT_CSV)\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    process_videos()"
      ],
      "metadata": {
        "id": "Tk3Fqz1ORJ2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Final FIXED VERSION ‚Äî Ensures transcript is always in one line, fully CSV-safe\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "# --- Step 1: File paths ---\n",
        "META_CSV = \"/content/youtube_videos_with_channel.csv\"         # Metadata file\n",
        "TRANSCRIPT_CSV = \"/content/latest_video_transcripts.csv\"  # Transcript file\n",
        "FINAL_CSV = \"Final_YouTube_Dataset.csv\"    # Output file\n",
        "\n",
        "# --- Step 2: Load both CSVs ---\n",
        "print(\"üìÇ Loading CSV files...\")\n",
        "meta_df = pd.read_csv(META_CSV)\n",
        "trans_df = pd.read_csv(TRANSCRIPT_CSV)\n",
        "\n",
        "# --- Step 3: Clean transcript text thoroughly ---\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    # Remove all kinds of line breaks, tabs, and excessive spaces\n",
        "    text = re.sub(r'[\\r\\n\\t\\u2028\\u2029]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "trans_df[\"text\"] = trans_df[\"text\"].apply(clean_text)\n",
        "\n",
        "# --- Step 4: Merge metadata with transcripts ---\n",
        "print(\"üîÑ Merging metadata with transcripts...\")\n",
        "final_df = pd.merge(meta_df, trans_df, left_on=\"Video ID\", right_on=\"video_id\", how=\"left\")\n",
        "\n",
        "# --- Step 5: Drop duplicate column ---\n",
        "final_df.drop(columns=[\"video_id\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# --- Step 6: Add 'is_transcript_available' column ---\n",
        "final_df[\"is_transcript_available\"] = final_df[\"text\"].apply(\n",
        "    lambda x: bool(isinstance(x, str) and x and \"No transcript available\" not in x)\n",
        ")\n",
        "\n",
        "# --- Step 7: Reorder columns (availability before transcript) ---\n",
        "cols = list(final_df.columns)\n",
        "if \"is_transcript_available\" in cols and \"text\" in cols:\n",
        "    cols.remove(\"is_transcript_available\")\n",
        "    cols.insert(cols.index(\"text\"), \"is_transcript_available\")\n",
        "    final_df = final_df[cols]\n",
        "\n",
        "# --- Step 8: Save final CSV safely with all fields quoted ---\n",
        "final_df.to_csv(FINAL_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
        "\n",
        "# --- Step 9: Show preview and download file ---\n",
        "print(\"\\n‚úÖ Final dataset created successfully!\")\n",
        "print(f\"üìÅ Saved as: {FINAL_CSV}\\n\")\n",
        "print(\"üîç Preview of cleaned dataset:\")\n",
        "print(final_df.head())\n",
        "\n",
        "files.download(FINAL_CSV)"
      ],
      "metadata": {
        "id": "YWMILBxATwIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
